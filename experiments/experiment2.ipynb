{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This id is used as a prefix for the figure names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 'experiment2'\n",
    "from datetime import datetime\n",
    "exp_id += '_' + str(datetime.now()).replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_graph\n",
    "from graph_loaders import load_graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from approx import GBFGreedy, GBFPointSelection\n",
    "from kernels import VarSpline, Diffusion\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "# G = load_graph('sensor2', path=path)\n",
    "# G = load_graph('sensor1', path=path)\n",
    "# G = load_graph('emptyset', path=path)\n",
    "# G = load_graph('2moon', path=path)\n",
    "# G = load_graph('minnesota', path=path)\n",
    "# G = load_graph('rand', path=path)\n",
    "# G = load_graph('rand_sparse', path=path)\n",
    "G = load_graph('bunny', path=path)\n",
    "\n",
    "# G = nx.dorogovtsev_goltsev_mendes_graph(7)\n",
    "# pos = nx.spectral_layout(G, center=[0.5, 0.5])\n",
    "# nx.set_node_attributes(G, pos, 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an optimization set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(len(G))\n",
    "y_train = np.ones(len(G))\n",
    "\n",
    "f = lambda x: np.exp(-(4 * np.linalg.norm(x - [.5, .5], axis=1)) ** 2)\n",
    "pos = np.array([[pos[0], pos[1]] for pos in nx.get_node_attributes(G, 'pos').values()])\n",
    "y_train = np.array(f(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the common params for the point selection and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20 # Max number of point to be selected\n",
    "tol_p = 1e-14  # Tolerance on the max of the squared power function\n",
    "tol_f = 1e-14  # Tolerance on the residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def max_error(y_true, y_pred):\n",
    "    return np.max(np.abs(y_true - y_pred))\n",
    "\n",
    "def max_variance(y_true, y_pred):\n",
    "    return np.max(np.abs(y_pred))\n",
    "\n",
    "scorer = make_scorer(mean_error, greater_is_better=False)\n",
    "\n",
    "cv = 5          # cv-fold cross validation\n",
    "n_jobs = -1     # number of parallel jobs (-1: all available cores)\n",
    "grid_size = 25   # size of 1d discretization grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the points for non-optimized kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = {}\n",
    "kernel['var_spline'] = VarSpline(G, par=[-1, 0.01])\n",
    "kernel['diffusion'] = Diffusion(G, par=[-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_opt_models = {}\n",
    "for ker_id in kernel:\n",
    "    print('Training model with ' + str(kernel[ker_id]))\n",
    "    non_opt_models[ker_id] = GBFGreedy(G, kernel=kernel[ker_id], greedy_type='p_greedy', \n",
    "                      reg_par=0, \n",
    "                      max_iter=max_iter, tol_p=tol_p, tol_f=tol_f,\n",
    "                      verbose=False)\n",
    "    non_opt_models[ker_id].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the points for optimized kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['var_spline'] = {\n",
    "        'kernel': ['VarSpline'],\n",
    "        'kernel_par': [[-x, y] for x in np.logspace(-1, 1, grid_size) for y in np.logspace(-16, 0, grid_size)]\n",
    "        }\n",
    "\n",
    "params['diffusion'] = {\n",
    "        'kernel': ['Diffusion'],\n",
    "        'kernel_par': [[-x] for x in np.logspace(-2, 2, grid_size)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_models = {}\n",
    "for ker_id in kernel:\n",
    "    print('Training optimized model with ' + str(kernel[ker_id]))\n",
    "#     opt_models[ker_id] = GridSearchCV(GBFGreedy(G, greedy_type='p_greedy', \n",
    "#                                                 reg_par=0, \n",
    "#                                                 max_iter=max_iter, tol_p=tol_p, tol_f=tol_f, \n",
    "#                                                 verbose=False), \n",
    "#                                       param_grid=params[ker_id], scoring=scorer, n_jobs=n_jobs, \n",
    "#                                       cv=cv, refit=True, verbose=1)\n",
    "    opt_models[ker_id] = GridSearchCV(GBFPointSelection(G, greedy_type='p_greedy', \n",
    "                                                reg_par=0, \n",
    "                                                max_iter=max_iter, tol_p=tol_p, tol_f=tol_f, \n",
    "                                                verbose=False), \n",
    "                                      param_grid=params[ker_id], scoring=scorer, n_jobs=n_jobs, \n",
    "                                      cv=cv, refit=True, verbose=1)\n",
    "\n",
    "    opt_models[ker_id].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not optimized:')\n",
    "for ker_id in kernel:\n",
    "    print(non_opt_models[ker_id].kernel)\n",
    "    \n",
    "print('\\nOptimized:')\n",
    "for ker_id in kernel:\n",
    "    print(opt_models[ker_id].best_estimator_.kernel)\n",
    "for ker_id in kernel:\n",
    "    print(opt_models[ker_id].best_estimator_.kernel.par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for idx, ker_id in enumerate(kernel):\n",
    "    ax = plt.subplot(2, len(kernel), idx+1) \n",
    "    plot_graph(G, ax=ax, values=non_opt_models[ker_id].eval_power_fun(X_train), \n",
    "               nodelist=non_opt_models[ker_id].ctrs_, \n",
    "               cb_label='Standard deviation')\n",
    "    ax.set_title(ker_id)\n",
    "    \n",
    "    ax = plt.subplot(2, len(kernel), len(kernel)+idx+1) \n",
    "    plot_graph(G, ax=ax, values=opt_models[ker_id].best_estimator_.eval_power_fun(X_train), \n",
    "               nodelist=opt_models[ker_id].best_estimator_.ctrs_, \n",
    "               cb_label='Standard deviation')\n",
    "    ax.set_title('optimized ' + ker_id)\n",
    "    \n",
    "plt.savefig('figures/' + exp_id + '_points' + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the decay of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.gca()\n",
    "leg = []\n",
    "\n",
    "for idx, ker_id in enumerate(kernel):\n",
    "    ax.semilogy(non_opt_models[ker_id].train_hist['p'] / np.max(non_opt_models[ker_id].train_hist['p']), 'o-')\n",
    "    leg.append(ker_id)\n",
    "    \n",
    "    ax.semilogy(opt_models[ker_id].best_estimator_.train_hist['p'] / np.max(opt_models[ker_id].best_estimator_.train_hist['p']), 'o-')\n",
    "    leg.append('optimized ' + ker_id)\n",
    "\n",
    "ax.legend(leg, fontsize=16)\n",
    "ax.set_xlabel('Number of nodes', fontsize=16)\n",
    "ax.set_ylabel('Max. standard deviation', fontsize=16)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig('figures/' + exp_id + '_p_max' + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the decay of the residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca()\n",
    "leg = []\n",
    "\n",
    "for idx, ker_id in enumerate(kernel):\n",
    "    ax.semilogy(non_opt_models[ker_id].train_hist['f'])\n",
    "    leg.append(ker_id)\n",
    "    ax.semilogy(opt_models[ker_id].best_estimator_.train_hist['f'])\n",
    "    leg.append('optimized ' + ker_id)\n",
    "\n",
    "ax.legend(leg, fontsize=16)\n",
    "ax.set_xlabel('Number of nodes', fontsize=16)\n",
    "ax.set_ylabel('Max. residual', fontsize=16)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig('figures/' + exp_id + '_f_max' + '.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the order of the first selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {}\n",
    "\n",
    "for ker_id in kernel:\n",
    "    data[ker_id] = non_opt_models[ker_id].ctrs_.flatten()\n",
    "    \n",
    "for ker_id in kernel:\n",
    "    data['optimized ' + ker_id] = opt_models[ker_id].best_estimator_.ctrs_.flatten()\n",
    "\n",
    "points = pd.DataFrame(data)\n",
    "points.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
