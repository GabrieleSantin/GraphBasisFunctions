{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to the basic computation of a GBF approximant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from utils import plot_graph\n",
    "from graph_loaders import load_graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from approx import GBFInterpolant\n",
    "from kernels import VarSpline, Diffusion\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by loading a pre-defined graph to be used as an example. \n",
    "\n",
    "All the following graphs have coordinate information for each node (as an attribute `pos` scaled to `[0, 1]^2`) that is used for visualization purposes. However, this information is not necessary nor used in the approximation process, since the main code only assumes that `G` is a `networkx` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# G = load_graph('wbc')\n",
    "# G = load_graph('sensor2')\n",
    "# G = load_graph('sensor1')\n",
    "# G = load_graph('emptyset')\n",
    "# G = load_graph('2moon')\n",
    "# G = load_graph('minnesota')\n",
    "# G = load_graph('rand')\n",
    "# G = load_graph('rand_sparse')\n",
    "G = load_graph('bunny')\n",
    "\n",
    "# G = nx.dorogovtsev_goltsev_mendes_graph(7)\n",
    "# pos = nx.spectral_layout(G, center=[0.5, 0.5])\n",
    "# nx.set_node_attributes(G, pos, 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a training and a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We define a signal/function on the nodes using the `pos` attribute. This is an interesting test as the approximation process does not have access to this attribute, and it tries to reconstruct the signal by using only information on the nodes' connectivity.\n",
    "\n",
    "The signal `f` is defined as a Gaussian centered and scaled around the mean point of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: np.exp(-(4 * np.linalg.norm(x - [.5, .5], axis=1)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We extract a random subset of 10% of the nodes to be used as the training set, and as a test set we use the entire set of nodes. All nodes sets are represented by the list of their indices in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n_train = int(len(G) * 0.1)\n",
    "X_train = np.random.randint(1, len(G), size=n_train)\n",
    "X_test = np.arange(len(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we assign the train and test values by evaluating `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pos = np.array([[pos[0], pos[1]] for pos in nx.get_node_attributes(G, 'pos').values()])\n",
    "\n",
    "y_test = np.array(f(pos))\n",
    "y_train = y_test[X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The signal looks as follows. The training nodes are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "plot_graph(G, ax=ax, values=y_test, nodelist=X_train, \n",
    "           cb_label='Target signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now pick a Graph Basis Function as the kernel that will be used in the approximation. \n",
    "Kernels need to be implementations of the abstract class `GraphKernel` that is defined in `kernel.py`. The file also contains the implementation of some concrete kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# kernel = VarSpline(G, par=[-1.1, 0.01])\n",
    "kernel = Diffusion(G, par=[-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We visualize a kernel translate into one of the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "idx = int(np.random.randint(0, len(X_train), 1))\n",
    "ker_eval = kernel.eval(X_test, X_train[idx])\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "plot_graph(G, ax=ax, values=ker_eval, nodelist=X_train[idx:idx+1], \n",
    "          cb_label='A GBF translate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstruct the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first initialize the approximant. Several approximants are available, and they are all implementations of the abstract class `GBFApprox` (see `approx.py`).\n",
    "\n",
    "These methods may be initialized by passing a `GraphKernel` object, or a string and a list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = GBFInterpolant(G, kernel=kernel, reg_par=1e-12)\n",
    "\n",
    "# Or:\n",
    "# model = GBFInterpolant(G, kernel='Diffusion', reg_par=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe that also a `kernel_par` variable can be explicitly passed, and by doing so the parameters of an existing kernel are overwritten, but only when calling the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "kernel_tmp = Diffusion(G, par=[-10])\n",
    "print('Before: ' + str(kernel_tmp))\n",
    "model_tmp = GBFInterpolant(G, kernel=kernel_tmp, kernel_par=[-1], reg_par=1e-12)\n",
    "print('After:  ' + str(model_tmp.kernel))\n",
    "model_tmp.fit([0], [0]) # Fit with dummy data\n",
    "print('After fit:  ' + str(model_tmp.kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now fit the approximant to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that the model is trained, we can compute the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And compute some errors. We use a clipping in the computation of the relative error to avoid dividing by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rel_err_tol = 1e-10\n",
    "abs_err_test = np.abs(y_test - s_test)\n",
    "rel_err_test = abs_err_test / np.clip(np.abs(y_test), rel_err_tol, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we visualize some results: the original and the reconstructed signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plot_graph(G, ax=ax, values=y_test, nodelist=model.ctrs_, \n",
    "           cb_label='Target signal')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plot_graph(G, ax=ax, values=s_test, nodelist=model.ctrs_, \n",
    "           cb_label='Reconstructed signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the absolute and relative test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plot_graph(G, ax=ax, values=abs_err_test, nodelist=model.ctrs_, \n",
    "           cb_label='Absolute Error', log_scale=True)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plot_graph(G, ax=ax, values=rel_err_test, nodelist=model.ctrs_, \n",
    "           cb_label='Relative Error (clipped to %2.2e)' % rel_err_tol, log_scale=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
