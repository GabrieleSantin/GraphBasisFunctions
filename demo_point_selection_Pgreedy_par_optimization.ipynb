{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the point selection via P-greedy with parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_graph\n",
    "from graph_loaders import load_graph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from approx import GBFGreedy\n",
    "from kernels import VarSpline, Diffusion\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading a pre-defined graph to be used as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = load_graph('wbc')\n",
    "# G = load_graph('sensor2')\n",
    "# G = load_graph('sensor1'\n",
    "# G = load_graph('emptyset')\n",
    "# G = load_graph('2moon')\n",
    "# G = load_graph('minnesota')\n",
    "# G = load_graph('rand')\n",
    "# G = load_graph('rand_sparse')\n",
    "G = load_graph('bunny')\n",
    "# G = load_graph('star')\n",
    "\n",
    "# G = nx.dorogovtsev_goltsev_mendes_graph(7)\n",
    "# pos = nx.spectral_layout(G, center=[0.5, 0.5])\n",
    "# nx.set_node_attributes(G, pos, 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an optimization set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the focus is on point selection only. This means that we can use all the nodes as a training set, but without the need to have target values `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(len(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a costant vector of ones as a target. This is used to choose the parameters in the next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.ones(len(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal looks as follows. The training nodes are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the parameters and reconstruct the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a metric to rank the performances of the different parameters. In this case the best model is the one providing the smallest mean error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "scorer = make_scorer(mean_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 5\n",
    "\n",
    "# Grid for the Diffusion kernel\n",
    "kernel = [['Diffusion']]\n",
    "reg_par = [[x] for x in np.logspace(-15, 0, n_grid)]\n",
    "kernel_par = [[-x] for x in np.logspace(-1, 2, n_grid)]\n",
    "params_1 = [{'kernel': kernel, 'reg_par' : reg_par, 'kernel_par': kernel_par}\n",
    "                   for kernel, reg_par, kernel_par in product(kernel, reg_par, kernel_par)]\n",
    "\n",
    "# Grid for the VarSpline kernel\n",
    "kernel = [['VarSpline']]\n",
    "reg_par = [[x] for x in np.logspace(-15, 0, n_grid)]\n",
    "kernel_par = [[-x, y] for x in np.logspace(-1, 2, 5) for y in np.linspace(0, 10, n_grid)]\n",
    "params_2 = [{'kernel': kernel, 'reg_par' : reg_par, 'kernel_par': kernel_par}\n",
    "                   for kernel, reg_par, kernel_par in product(kernel, reg_par, kernel_par)]\n",
    "\n",
    "# Join the two grids\n",
    "params = params_1 + params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we wrap the approximation model into a `GridSearchCV`. We use all the available cores and run `cv=5`-fold cross validation, with final refitting. Here we turn off the regularization (i.e., `reg_par=0`) since we are interested purely in the variance minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100 # Max number of point to be selected\n",
    "tol_p = 1e-12  # Tolerance on the max of the squared power function\n",
    "tol_f = 1e-12      # Tolerance on the residual\n",
    "\n",
    "model = GridSearchCV(GBFGreedy(G, greedy_type='p_greedy', \n",
    "                               max_iter=max_iter, tol_p=tol_p, \n",
    "                               verbose=False), \n",
    "                     params, scoring=scorer, n_jobs=6, cv=5, \n",
    "                     refit=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the approximant to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the selected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the selected points and the decay of the power function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_max = model.best_estimator_.train_hist['p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate the algebraic rate of decay of the power function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_size = int(0.3 * len(p_max)) \n",
    "nn = np.arange(1, len(p_max) + 1)\n",
    "coeff_max = np.polyfit(np.log(nn)[-tail_size:], np.log(p_max)[-tail_size:], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca()\n",
    "a = ax.loglog(p_max)\n",
    "ax.loglog(nn[-2*tail_size:], np.exp(coeff_max[1]) * nn[-2*tail_size:] ** coeff_max[0], \n",
    "              '--', color=a[0].get_color())\n",
    "ax.legend(['Max of the power function', '$n^{%2.2f}$' % coeff_max[0]], fontsize=16, loc=(1.1, 0.1))\n",
    "ax.set_xlabel('Number of nodes', fontsize=16)\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(16) \n",
    "ax.grid(True)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the selected points. In this case we visualize the power function values as a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.best_estimator_.eval_power_fun(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca()\n",
    "plot_graph(G, ax=ax, values=p, nodelist=model.best_estimator_.ctrs_, \n",
    "           cb_label='Power function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize also the error with respect to the constant signal with all ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train = model.predict(X_train)\n",
    "abs_err_test = np.abs(y_train - s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "plot_graph(G, ax=ax, values=abs_err_test, nodelist=model.best_estimator_.ctrs_, \n",
    "           cb_label='Absolute Error', log_scale=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
